# -*- coding: utf-8 -*-
"""Fake News Detecion (New Dataset).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iKblJBBU_OOO0kOmWm1wfL68lJvJ-Ygs
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Needed for Lingustic Analysis
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# We will visualize the messages with a word cloud
from wordcloud import WordCloud

# Multinomial Naive Bayes Classifier
from sklearn.naive_bayes import MultinomialNB

#KNN Classifier
from sklearn.neighbors import KNeighborsClassifier

# Import Tf-idf Vectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Import the Label Encoder
from sklearn.preprocessing import LabelEncoder

# Import the train test split
from sklearn.model_selection import train_test_split
from pprint import pprint
# To evaluate our model
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import label_binarize

import nltk 

nltk.download('all')

data_fake = pd.read_csv("Fake.csv")
data_true = pd.read_csv("True.csv")

data_fake.head()

data_true.head(5)

# Inserting a column as the Target
data_fake["class"] = 0
data_true["class"] = 1

data_fake.shape, data_true.shape

data = pd.concat([data_fake, data_true], axis = 0)
data.shape

data.columns

df = data.drop(["title", "subject","date"], axis = 1)

# To check if there are any missing values in the dataset
df.isnull().sum()

# To random shuffle the dataset
df = df.sample(frac = 1)

df.head()

# To reset the index of the numbers (start from 1 all over again)
df.reset_index(inplace = True)
df.drop(["index"], axis = 1, inplace = True)

df.columns

df.head()

# Print the counts of each category
print(df['class'].value_counts())

# Print the proportions of each category
print(df['class'].value_counts(normalize=True))

# Visualize the Categories
sns.countplot(df['class'])
plt.title("Class Counts")
plt.show()

# Store the number of words in each messages
df['word_count'] = df['text'].str.split().str.len()

# Print the average number of words in each category
print(df.groupby('class')['word_count'].mean())

# Our dataset is balanced between all five categories

# Visualize the distribution of word counts in each category
sns.distplot(df[df['class']==1]['word_count'], label='True')
sns.distplot(df[df['class']==0]['word_count'], label='False')
plt.legend()
plt.show()

# Make the letters lower case and tokenize the words
tokenized_text = df['text'].str.lower().apply(word_tokenize)

# Print the tokens to see how it looks like
print(tokenized_text)

# Define a function to returns only alphanumeric tokens
def alpha(tokens):
    """This function removes all non-alphanumeric characters"""
    alpha = []
    for token in tokens:
        if str.isalpha(token) or token in ['n\'t','won\'t']:
            if token=='n\'t':
                alpha.append('not')
                continue
            elif token == 'won\'t':
                alpha.append('wont')
                continue
            alpha.append(token)
    return alpha

# Apply our function to tokens
tokenized_text = tokenized_text.apply(alpha)

print(tokenized_text)

# Define a function to remove stop words
def remove_stop_words(tokens):
    """This function removes all stop words in terms of nltk stopwords"""
    no_stop = []
    for token in tokens:
        if token not in stopwords.words('english'):
            no_stop.append(token)
    return no_stop

# Apply our function to tokens
tokenized_text = tokenized_text.apply(remove_stop_words)

print(tokenized_text)

# Define a function to lemmatization
def lemmatize(tokens):
    """This function lemmatize the messages"""
    # Initialize the WordNetLemmatizer
    lemmatizer = WordNetLemmatizer()
    # Create the lemmatized list
    lemmatized = []
    for token in tokens:
            # Lemmatize and append
            lemmatized.append(lemmatizer.lemmatize(token))
    return " ".join(lemmatized)

# Apply our function to tokens
tokenized_text = tokenized_text.apply(lemmatize)

print(tokenized_text)

# Replace the columns with tokenized messages
df['text'] = tokenized_text

# Display the first five rows
display(df.head(10))

# Get the true text
true = data[data['class']==0]['text'].str.cat(sep=', ')

# Get the ham messages
false = data[data['class']==1]['text'].str.cat(sep=', ')

# Initialize the word cloud
wc = WordCloud(width = 500, height = 500, min_font_size = 10, background_color ='white')

# Generate the world clouds for each type of message
true_wc = wc.generate(true)

# plot the world cloud for true                     
plt.figure(figsize = (5, 5), facecolor = None) 
plt.imshow(true_wc) 
plt.axis("off") 
plt.title("Common words in text labeled as true")
plt.tight_layout(pad = 0) 
plt.show() 

false_wc = wc.generate(false)
# plot the world cloud for spam                       
plt.figure(figsize = (5, 5), facecolor = None) 
plt.imshow(false_wc) 
plt.axis("off")
plt.title("Common words in text labeled as false")
plt.tight_layout(pad = 0) 
plt.show()

# Select the features and the target
X = data['text']
y = data['class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=34, stratify=y)

# Create the tf-idf vectorizer
vectorizer = TfidfVectorizer(strip_accents='ascii')

# First fit the vectorizer with our training set
tfidf_train = vectorizer.fit_transform(X_train)

# Now we can fit our test data with the same vectorizer
tfidf_test = vectorizer.transform(X_test)

# Initialize the Multinomial Naive Bayes classifier
nb = MultinomialNB()

# Fit the model
nb.fit(tfidf_train, y_train)

# Print the accuracy score
print("Accuracy:",nb.score(tfidf_test, y_test))

# Predict the labels
y_pred = nb.predict(tfidf_test)

# Print the Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix For Naive Bayes\n")
print(cm)

# Print the Classification Report
cr = classification_report(y_test, y_pred)
print("\n\nClassification Report For Bayes\n")
print(cr)


# Print the Receiver operating characteristic Auc score
auc_score = roc_auc_score(y_test, y_pred)
print("\nROC AUC Score:",auc_score)

# Get probabilities.
y_pred_proba = nb.predict(tfidf_test)

# Get False Positive rate, True Positive rate and the threshold
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# Visualize the ROC curve.
plt.plot(fpr, tpr)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('FP Rate')
plt.ylabel('TP Rate')
plt.title('ROC')
plt.show()

