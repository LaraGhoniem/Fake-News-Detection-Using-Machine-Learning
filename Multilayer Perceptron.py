
"""Fake News Detecion (New Dataset).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iKblJBBU_OOO0kOmWm1wfL68lJvJ-Ygs
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer


from wordcloud import WordCloud


from sklearn.naive_bayes import MultinomialNB


from sklearn.neighbors import KNeighborsClassifier


from sklearn.feature_extraction.text import TfidfVectorizer


from sklearn.preprocessing import LabelEncoder


from sklearn.model_selection import train_test_split
from pprint import pprint

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import roc_auc_score
from sklearn.preprocessing import label_binarize

import nltk 

nltk.download('all')

data_fake = pd.read_csv("Fake.csv")
data_true = pd.read_csv("True.csv")

data_fake.head()

data_true.head(5)


data_fake["class"] = 0
data_true["class"] = 1

data_fake.shape, data_true.shape

data = pd.concat([data_fake, data_true], axis = 0)
data.shape

data.columns

df = data.drop(["title", "subject","date"], axis = 1)


df.isnull().sum()


df = df.sample(frac = 1)

df.head()


df.reset_index(inplace = True)
df.drop(["index"], axis = 1, inplace = True)

df.columns

df.head()


print(df['class'].value_counts())


print(df['class'].value_counts(normalize=True))


sns.countplot(df['class'])
plt.title("Class Counts")
plt.show()


df['word_count'] = df['text'].str.split().str.len()


print(df.groupby('class')['word_count'].mean())

sns.distplot(df[df['class']==1]['word_count'], label='True')
sns.distplot(df[df['class']==0]['word_count'], label='False')
plt.legend()
plt.show()


tokenized_text = df['text'].str.lower().apply(word_tokenize)


print(tokenized_text)


def alpha(tokens):
    """This function removes all non-alphanumeric characters"""
    alpha = []
    for token in tokens:
        if str.isalpha(token) or token in ['n\'t','won\'t']:
            if token=='n\'t':
                alpha.append('not')
                continue
            elif token == 'won\'t':
                alpha.append('wont')
                continue
            alpha.append(token)
    return alpha


tokenized_text = tokenized_text.apply(alpha)

print(tokenized_text)


def remove_stop_words(tokens):
    """This function removes all stop words in terms of nltk stopwords"""
    no_stop = []
    for token in tokens:
        if token not in stopwords.words('english'):
            no_stop.append(token)
    return no_stop


tokenized_text = tokenized_text.apply(remove_stop_words)

print(tokenized_text)


def lemmatize(tokens):
    """This function lemmatize the messages"""
    
    lemmatizer = WordNetLemmatizer()
    
    lemmatized = []
    for token in tokens:
            
            lemmatized.append(lemmatizer.lemmatize(token))
    return " ".join(lemmatized)


tokenized_text = tokenized_text.apply(lemmatize)

print(tokenized_text)


df['text'] = tokenized_text

print(df.head(10))


true = data[data['class']==0]['text'].str.cat(sep=', ')

false = data[data['class']==1]['text'].str.cat(sep=', ')


wc = WordCloud(width = 500, height = 500, min_font_size = 10, background_color ='white')


true_wc = wc.generate(true)


plt.figure(figsize = (5, 5), facecolor = None) 
plt.imshow(true_wc) 
plt.axis("off") 
plt.title("Common words in text labeled as true")
plt.tight_layout(pad = 0) 
plt.show() 

false_wc = wc.generate(false)               
plt.figure(figsize = (5, 5), facecolor = None) 
plt.imshow(false_wc) 
plt.axis("off")
plt.title("Common words in text labeled as false")
plt.tight_layout(pad = 0) 
plt.show()


X = data['text']
y = data['class']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=34, stratify=y)


vectorizer = TfidfVectorizer(strip_accents='ascii')


tfidf_train = vectorizer.fit_transform(X_train)


tfidf_test = vectorizer.transform(X_test)


nb = MultinomialNB()


nb.fit(tfidf_train, y_train)


print("Accuracy:",nb.score(tfidf_test, y_test))


y_pred = nb.predict(tfidf_test)


cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix For Naive Bayes\n")
print(cm)


cr = classification_report(y_test, y_pred)
print("\n\nClassification Report For Bayes\n")
print(cr)



auc_score = roc_auc_score(y_test, y_pred)
print("\nROC AUC Score:",auc_score)


y_pred_proba = nb.predict(tfidf_test)


fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)


plt.plot(fpr, tpr)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('FP Rate')
plt.ylabel('TP Rate')
plt.title('ROC')
plt.show()

